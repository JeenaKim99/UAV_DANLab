{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/100, Total Reward: -20\n",
      "Episode 2/100, Total Reward: -20\n",
      "Episode 3/100, Total Reward: -20\n",
      "Episode 4/100, Total Reward: -20\n",
      "Episode 5/100, Total Reward: -20\n",
      "Episode 6/100, Total Reward: -20\n",
      "Episode 7/100, Total Reward: -20\n",
      "Episode 8/100, Total Reward: -20\n",
      "Episode 9/100, Total Reward: -20\n",
      "Episode 10/100, Total Reward: -20\n",
      "Episode 11/100, Total Reward: -20\n",
      "Episode 12/100, Total Reward: -20\n",
      "Episode 13/100, Total Reward: -14\n",
      "Episode 14/100, Total Reward: -20\n",
      "Episode 15/100, Total Reward: -20\n",
      "Episode 16/100, Total Reward: -20\n",
      "Episode 17/100, Total Reward: -20\n",
      "Episode 18/100, Total Reward: -20\n",
      "Episode 19/100, Total Reward: -20\n",
      "Episode 20/100, Total Reward: -20\n",
      "Episode 21/100, Total Reward: -20\n",
      "Episode 22/100, Total Reward: -20\n",
      "Episode 23/100, Total Reward: -20\n",
      "Episode 24/100, Total Reward: -20\n",
      "Episode 25/100, Total Reward: -20\n",
      "Episode 26/100, Total Reward: -20\n",
      "Episode 27/100, Total Reward: -20\n",
      "Episode 28/100, Total Reward: -20\n",
      "Episode 29/100, Total Reward: -20\n",
      "Episode 30/100, Total Reward: -20\n",
      "Episode 31/100, Total Reward: -20\n",
      "Episode 32/100, Total Reward: -20\n",
      "Episode 33/100, Total Reward: -20\n",
      "Episode 34/100, Total Reward: -20\n",
      "Episode 35/100, Total Reward: -20\n",
      "Episode 36/100, Total Reward: -20\n",
      "Episode 37/100, Total Reward: -20\n",
      "Episode 38/100, Total Reward: -20\n",
      "Episode 39/100, Total Reward: -20\n",
      "Episode 40/100, Total Reward: -20\n",
      "Episode 41/100, Total Reward: -20\n",
      "Episode 42/100, Total Reward: -20\n",
      "Episode 43/100, Total Reward: -20\n",
      "Episode 44/100, Total Reward: -20\n",
      "Episode 45/100, Total Reward: -20\n",
      "Episode 46/100, Total Reward: -20\n",
      "Episode 47/100, Total Reward: -20\n",
      "Episode 48/100, Total Reward: -20\n",
      "Episode 49/100, Total Reward: -20\n",
      "Episode 50/100, Total Reward: -20\n",
      "Episode 51/100, Total Reward: -20\n",
      "Episode 52/100, Total Reward: -20\n",
      "Episode 53/100, Total Reward: -20\n",
      "Episode 54/100, Total Reward: -20\n",
      "Episode 55/100, Total Reward: -20\n",
      "Episode 56/100, Total Reward: -20\n",
      "Episode 57/100, Total Reward: -20\n",
      "Episode 58/100, Total Reward: -20\n",
      "Episode 59/100, Total Reward: -20\n",
      "Episode 60/100, Total Reward: -20\n",
      "Episode 61/100, Total Reward: -20\n",
      "Episode 62/100, Total Reward: -20\n",
      "Episode 63/100, Total Reward: -20\n",
      "Episode 64/100, Total Reward: -20\n",
      "Episode 65/100, Total Reward: -20\n",
      "Episode 66/100, Total Reward: -20\n",
      "Episode 67/100, Total Reward: -20\n",
      "Episode 68/100, Total Reward: -20\n",
      "Episode 69/100, Total Reward: -20\n",
      "Episode 70/100, Total Reward: -20\n",
      "Episode 71/100, Total Reward: -20\n",
      "Episode 72/100, Total Reward: -20\n",
      "Episode 73/100, Total Reward: -20\n",
      "Episode 74/100, Total Reward: -20\n",
      "Episode 75/100, Total Reward: -20\n",
      "Episode 76/100, Total Reward: -20\n",
      "Episode 77/100, Total Reward: -20\n",
      "Episode 78/100, Total Reward: -20\n",
      "Episode 79/100, Total Reward: -20\n",
      "Episode 80/100, Total Reward: -20\n",
      "Episode 81/100, Total Reward: -20\n",
      "Episode 82/100, Total Reward: -20\n",
      "Episode 83/100, Total Reward: -20\n",
      "Episode 84/100, Total Reward: -20\n",
      "Episode 85/100, Total Reward: -20\n",
      "Episode 86/100, Total Reward: -20\n",
      "Episode 87/100, Total Reward: -20\n",
      "Episode 88/100, Total Reward: -20\n",
      "Episode 89/100, Total Reward: -4\n",
      "Episode 90/100, Total Reward: -20\n",
      "Episode 91/100, Total Reward: -20\n",
      "Episode 92/100, Total Reward: -20\n",
      "Episode 93/100, Total Reward: -20\n",
      "Episode 94/100, Total Reward: -20\n",
      "Episode 95/100, Total Reward: -20\n",
      "Episode 96/100, Total Reward: -6\n",
      "Episode 97/100, Total Reward: -20\n",
      "Episode 98/100, Total Reward: -20\n",
      "Episode 99/100, Total Reward: -20\n",
      "Episode 100/100, Total Reward: -20\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[-20,\n",
       " -20,\n",
       " -20,\n",
       " -20,\n",
       " -20,\n",
       " -20,\n",
       " -20,\n",
       " -20,\n",
       " -20,\n",
       " -20,\n",
       " -20,\n",
       " -20,\n",
       " -14,\n",
       " -20,\n",
       " -20,\n",
       " -20,\n",
       " -20,\n",
       " -20,\n",
       " -20,\n",
       " -20,\n",
       " -20,\n",
       " -20,\n",
       " -20,\n",
       " -20,\n",
       " -20,\n",
       " -20,\n",
       " -20,\n",
       " -20,\n",
       " -20,\n",
       " -20,\n",
       " -20,\n",
       " -20,\n",
       " -20,\n",
       " -20,\n",
       " -20,\n",
       " -20,\n",
       " -20,\n",
       " -20,\n",
       " -20,\n",
       " -20,\n",
       " -20,\n",
       " -20,\n",
       " -20,\n",
       " -20,\n",
       " -20,\n",
       " -20,\n",
       " -20,\n",
       " -20,\n",
       " -20,\n",
       " -20,\n",
       " -20,\n",
       " -20,\n",
       " -20,\n",
       " -20,\n",
       " -20,\n",
       " -20,\n",
       " -20,\n",
       " -20,\n",
       " -20,\n",
       " -20,\n",
       " -20,\n",
       " -20,\n",
       " -20,\n",
       " -20,\n",
       " -20,\n",
       " -20,\n",
       " -20,\n",
       " -20,\n",
       " -20,\n",
       " -20,\n",
       " -20,\n",
       " -20,\n",
       " -20,\n",
       " -20,\n",
       " -20,\n",
       " -20,\n",
       " -20,\n",
       " -20,\n",
       " -20,\n",
       " -20,\n",
       " -20,\n",
       " -20,\n",
       " -20,\n",
       " -20,\n",
       " -20,\n",
       " -20,\n",
       " -20,\n",
       " -20,\n",
       " -4,\n",
       " -20,\n",
       " -20,\n",
       " -20,\n",
       " -20,\n",
       " -20,\n",
       " -20,\n",
       " -6,\n",
       " -20,\n",
       " -20,\n",
       " -20,\n",
       " -20]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 필요한 패키지 및 모듈 임포트\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "\n",
    "# SmallUAVEnv 클래스 정의\n",
    "class SmallUAVEnv:\n",
    "    def __init__(self):\n",
    "        self.position = [0, 0]\n",
    "        self.goal = [3, 3]  # Smaller grid\n",
    "        self.actions = [(0, 1), (1, 0), (0, -1), (-1, 0)]  # Up, Right, Down, Left\n",
    "\n",
    "    def reset(self):\n",
    "        self.position = [0, 0]\n",
    "        return self.position\n",
    "\n",
    "    def step(self, action):\n",
    "        # Update position\n",
    "        self.position[0] += self.actions[action][0]\n",
    "        self.position[1] += self.actions[action][1]\n",
    "\n",
    "        # Check if goal is reached\n",
    "        if self.position == self.goal:\n",
    "            reward = +1\n",
    "            done = True\n",
    "        else:\n",
    "            reward = -1\n",
    "            done = False\n",
    "\n",
    "        return self.position, reward, done\n",
    "\n",
    "# SmallUAVEnv 객체 생성\n",
    "small_env = SmallUAVEnv()\n",
    "\n",
    "# SimpleDQN 클래스 정의\n",
    "class SimpleDQN(nn.Module):\n",
    "    def __init__(self, input_dim, num_actions):\n",
    "        super(SimpleDQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, num_actions)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "\n",
    "# 학습 함수 train_dqn_quick_v2 수정: 보상 출력\n",
    "def train_dqn_quick_v2_with_rewards(env, model, episodes=1000, gamma=0.99, epsilon=0.1, lr=0.0001, max_steps=50):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "    num_actions = 4\n",
    "\n",
    "    # To store rewards for each episode\n",
    "    episode_rewards = []\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        step_count = 0\n",
    "        total_reward = 0  # Initialize total reward for this episode\n",
    "        while not done and step_count < max_steps:\n",
    "            if random.uniform(0, 1) < epsilon:\n",
    "                action = random.choice(range(num_actions))\n",
    "            else:\n",
    "                q_values = model(torch.tensor(state, dtype=torch.float32))\n",
    "                action = torch.argmax(q_values).item()\n",
    "\n",
    "            next_state, reward, done = env.step(action)\n",
    "            total_reward += reward  # Accumulate reward\n",
    "\n",
    "            target = reward + gamma * torch.max(model(torch.tensor(next_state, dtype=torch.float32)))\n",
    "            current_q = model(torch.tensor(state, dtype=torch.float32))[action]\n",
    "\n",
    "            loss = criterion(current_q, target.detach())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            state = next_state\n",
    "            step_count += 1\n",
    "        \n",
    "        # Append total reward for this episode to the list\n",
    "        episode_rewards.append(total_reward)\n",
    "        print(f\"Episode {episode + 1}/{episodes}, Total Reward: {total_reward}\")\n",
    "\n",
    "    return episode_rewards\n",
    "\n",
    "# 모델 객체 생성 및 학습 (with rewards printing)\n",
    "simple_model_with_rewards = SimpleDQN(input_dim=2, num_actions=4)\n",
    "rewards = train_dqn_quick_v2_with_rewards(small_env, simple_model_with_rewards, episodes=100, lr=0.05, max_steps=20)\n",
    "\n",
    "rewards\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
